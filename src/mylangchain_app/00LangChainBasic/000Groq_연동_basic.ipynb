{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello LangChain!!\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello LangChain!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k7JQaf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI \n",
    "\n",
    "#load_dotenv(dotenv_path='.env')\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(OPENAI_API_KEY[50:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n",
      "input_variables=['input'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='당신은 개발자입니다.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# prompt + llm + output \n",
    "\n",
    "# prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [ (\"system\", \"당신은 개발자입니다.\") , \n",
    "     (\"user\", \"{input}\") ]\n",
    ")\n",
    "print(type(prompt))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "System: 당신은 개발자입니다.\n",
      "Human: 파이썬은 무엇인가요? 자세하게 설명해주세요\n"
     ]
    }
   ],
   "source": [
    "prompt_text = prompt.format(input=\"파이썬은 무엇인가요? 자세하게 설명해주세요\")\n",
    "print(type(prompt_text))\n",
    "print(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_openai.chat_models.base.ChatOpenAI'>\n",
      "client=<openai.resources.chat.completions.completions.Completions object at 0x000002007D8DECF0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002007D97EA20> root_client=<openai.OpenAI object at 0x000002007D807680> root_async_client=<openai.AsyncOpenAI object at 0x000002007D97CAA0> model_name='moonshotai/kimi-k2-instruct-0905' temperature=0.7 model_kwargs={} openai_api_key=SecretStr('**********') openai_api_base='https://api.groq.com/openai/v1'\n"
     ]
    }
   ],
   "source": [
    "#llm = ChatOpenAI(api_key=OPENAI_API_KEY, model_name=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "# Groq API를 사용하는 ChatOpenAI 인스턴스 생성\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    #model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    model=\"moonshotai/kimi-k2-instruct-0905\",\n",
    "    # model=\"openai/gpt-oss-120b\",\n",
    "    temperature=0.7\n",
    ")\n",
    "print(type(llm))\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = llm.invoke(prompt_text)\n",
    "    print(type(response))\n",
    "    print(\"응답:\", response.content)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCEL\n",
    "### Prompt + LLM 모델 + OutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n",
      "first=ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='당신은 개발자입니다.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]) middle=[ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000002007D8DECF0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002007D97EA20>, root_client=<openai.OpenAI object at 0x000002007D807680>, root_async_client=<openai.AsyncOpenAI object at 0x000002007D97CAA0>, model_name='moonshotai/kimi-k2-instruct-0905', temperature=0.7, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://api.groq.com/openai/v1')] last=StrOutputParser()\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm | output_parser\n",
    "print(type(chain))\n",
    "print(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"input\":\"LangChain은 무엇인가요?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "LangChain은 “Large Language Model(LLM)을 애플리케이션에 쉽게 연결(chain)”해 주는 오픈소스 프레임워크입니다.  \n",
      "핵심은 “LLM을 단순히 채팅용으로 쓰는 게 아니라, 외부 데이터·도구·API와 반복적으로 연결해 복잡한 업무 자동화를 만든다”는 것입니다.\n",
      "\n",
      "1. 왜 나왔나?  \n",
      "   - GPT 같은 LLM은 지식이 고정돼 있고, 실시간 데이터·사내 DB·사용자 행동에는 접근 못 함  \n",
      "   - 복잡한 작업(“고객 질문 → DB 조회 → 보고서 생성 → 슬랙 전송”)은 한 번의 프롬프트로 안 됨  \n",
      "   → 이 두 문제를 “Chain(체인)”으로 풀자\n",
      "\n",
      "2. 핵심 개념 3줄 요약  \n",
      "   1) Components: LLM, 프롬프트 템플릿, 출력 파서, 메모리, Document Loader 등 레고 블록  \n",
      "   2) Chains: 블록들을 묶어 “Prompt → LLM → 후처리 → 다음 단계” 흐름을 반복  \n",
      "   3) Agents: LLM이 “지금 어떤 도구(tool)를 써야 할까?”를 스스로 판단하며 루프 실행\n",
      "\n",
      "3. 대표 모듈  \n",
      "   - langchain-core: 범용 인터페이스  \n",
      "   - langchain-community: 수백 개의 Loader/Tool 통합(Slack, Google, SQL, CSV…)  \n",
      "   - langchain-openai / anthropic / google 등 벤더별 어댑터  \n",
      "   - LangGraph: DAG·순환 워크플로 지원(2024 신규)  \n",
      "   - LangServe / LangSmith: REST 배포·모니터링·디버깅 플랫폼\n",
      "\n",
      "4. 실습 예 10초 컷  \n",
      "```python\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "from langchain.schema import SystemMessage, HumanMessage\n",
      "\n",
      "llm = ChatOpenAI()\n",
      "chain = (\n",
      "    SystemMessage(content=\"너는 친절한 한국어 비서야.\")\n",
      "    + HumanMessage(content=\"{질문}\")\n",
      "    | llm\n",
      ")\n",
      "print(chain.invoke({\"질문\": \"LangChain이 뭐야?\"}).content)\n",
      "```\n",
      "\n",
      "5. 고급 패턴  \n",
      "   - RAG: “웹 페이지 → 임베딩 → 벡터DB → 유사도 검색 → LLM 생성” 한 줄로 구성  \n",
      "   - ReAct: “생각→도구호출→관측→생각…” 루프로 수학 문제, API 조회, 브라우저 자동화  \n",
      "   - Plan-and-execute: 계획→실행→재계획… (복잡한 프로젝트 관리)\n",
      "\n",
      "6. LangChain이 아니면?  \n",
      "   - LlamaIndex: 검색·인덱싱에 특화, 코드가 더 짧음  \n",
      "   - Hugging Face Transformers Agents: 허브 모델을 바로 도구로  \n",
      "   - AutoGen, CrewAI: 멀티에이전트 대화 중심  \n",
      "   → 목적에 따라 선택/조합 가능\n",
      "\n",
      "7. 버전 변천  \n",
      "   - 0.0.1(2022.10) → 0.1(2023) 파이썬 중심  \n",
      "   - 0.2(2024) JS·Python 통합, Expression Language 도입  \n",
      "   - 향후 1.0 안정화 예정\n",
      "\n",
      "8. 요약  \n",
      "LangChain은 “LLM을 토대로 외부 세상과 반복적으로 상호작용하는 자동화 시스템”을 Python/JS로 빠르게 조립할 수 있게 도와주는 “레고 세트” 같은 프레임워크입니다.\n"
     ]
    }
   ],
   "source": [
    "print(type(response))\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mylangchain-app-tQs1zK9u-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
