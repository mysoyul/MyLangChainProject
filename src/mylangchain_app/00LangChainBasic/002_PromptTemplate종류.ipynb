{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PromptTemplate \n",
    "* [PromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html#langchain_core.prompts.prompt.PromptTemplate)\n",
    "* [ChatPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html#langchain_core.prompts.chat.ChatPromptTemplate)\n",
    "* [ChatMessagePromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatMessagePromptTemplate.html#langchain_core.prompts.chat.ChatMessagePromptTemplate)\n",
    "* [FewShotPromptTemplate](https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.few_shot.FewShotPromptTemplate.html#langchain_core.prompts.few_shot.FewShotPromptTemplate)\n",
    "* PartialPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poetry add python-dotenv langchain langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gsk_o\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# .env íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ì„œ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(OPENAI_API_KEY[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) PromptTemplate ì˜ from_template() í•¨ìˆ˜ ì‚¬ìš©\n",
    "* ì£¼ë¡œ LLM(í…ìŠ¤íŠ¸ ì™„ì„±í˜• ëª¨ë¸, ex. Ollama, GPT-3.5)ê³¼ í•¨ê»˜ ì‚¬ìš©\n",
    "* í•˜ë‚˜ì˜ ë¬¸ìì—´ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from pprint import pprint\n",
    "\n",
    "template_text = \"{model_name} ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ {count} ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\"\n",
    "\n",
    "# PromptTemplate ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "    #model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Spring AIì™€ ë™ì¼í•œ ëª¨ë¸\n",
    "    #model=\"openai/gpt-oss-120b\",\n",
    "    model=\"moonshotai/kimi-k2-instruct-0905\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "response = chain.invoke({\"model_name\":\"ChatGPT\", \"count\":3})\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) PromptTemplate ê²°í•©í•˜ê¸°\n",
    "* ë™ì¼í•œ Prompt íŒ¨í„´ì„ ì‚¬ìš©í•˜ì§€ë§Œ ì—¬ëŸ¬ ê°œì˜ ì§ˆë¬¸ì„ ì‘ì„±í•´ì„œ LLMì„ ì‹¤í–‰í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ChatGPTëŠ” ì¸í„°ë„·ì˜ ë°©ëŒ€í•œ í…ìŠ¤íŠ¸ë¥¼ í•™ìŠµí•´ ë‹¨ì–´Â·ë¬¸ì¥ì´ ë‚˜ì˜¬ í™•ë¥ ì„ ê³„ì‚°í•˜ê³ , ì‚¬ëŒì´ ì¤€ ì˜ˆì‹œì— ë§ì¶° ëŒ€ë‹µí•  í™•ë¥ ì„ ì¡°ì •í•˜ëŠ” '\n",
      " 'ë°©ì‹ìœ¼ë¡œ í•™ìŠµë©ë‹ˆë‹¤.  \\n'\n",
      " 'í•™ìŠµ ê³¼ì •ì—ì„œ â€œì´ ë¬¸ì¥ì´ ë‹¤ìŒì— ì˜¬ í™•ë¥ â€ì„ ì˜ˆì¸¡í•˜ë„ë¡ í›ˆë ¨ë˜ë©°, ì‚¬ëŒì˜ í”¼ë“œë°±(RLHF)ìœ¼ë¡œ ì„ í˜¸ë˜ëŠ” ëŒ€ë‹µì„ ì••ì¶•ì ìœ¼ë¡œ '\n",
      " 'ë§Œë“¤ì–´ê°‘ë‹ˆë‹¤.  \\n'\n",
      " 'ê²°êµ­ í†µê³„ì  íŒ¨í„´ì„ ê¸°ì–µí•˜ëŠ” ê±°ëŒ€í•œ â€œí™•ë¥  ê¸°ê³„â€ì´ë¯€ë¡œ ì‹¤ì‹œê°„ ê²€ìƒ‰Â·ê°ì • ì´í•´ëŠ” ë¶ˆì™„ì „í•˜ì§€ë§Œ, ë¬¸ë§¥ì— ë§ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ê¸€ì„ ë¹ ë¥´ê²Œ ë§Œë“¤ì–´ '\n",
      " 'ëƒ…ë‹ˆë‹¤.\\n'\n",
      " '\\n'\n",
      " 'ChatGPT ëª¨ë¸ì˜ ì¥ì   \\n'\n",
      " '- ë‹¤ì–‘í•œ ì£¼ì œì— ëŒ€í•´ ë…¼ë¦¬ì Â·ì°½ì˜ì  ê¸€ì“°ê¸°Â·ì½”ë“œ ì‘ì„±Â·ìš”ì•½Â·ë²ˆì—­ ë“±ì„ í•œ ë²ˆì— ìˆ˜í–‰  \\n'\n",
      " '- ëŒ€í™” ë§¥ë½ì„ ê¸°ì–µí•´ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ê°€ë©°, ì§ˆë¬¸ì„ ìˆ˜ì •í•˜ë©´ ê³§ë°”ë¡œ ë‹µë³€ ìŠ¤íƒ€ì¼Â·ê¸¸ì´Â·ë‚œì´ë„ ì¡°ì ˆ ê°€ëŠ¥  \\n'\n",
      " '- ë³„ë„ í•™ìŠµ ì—†ì´ few-shot ì˜ˆì‹œ ëª‡ ê°œë§Œ ì£¼ë©´ ìƒˆë¡œìš´ ì‘ì—…ì— ë¹ ë¥´ê²Œ ì ì‘(ì¶”ë¡  ì†ë„ ë¹ ë¦„, API í™œìš© í¸ë¦¬)\\n'\n",
      " '\\n'\n",
      " 'ChatGPTì™€ ë¹„ìŠ·í•œ AI ëª¨ë¸  \\n'\n",
      " '- êµ¬ê¸€ ë°”ë“œ(íŒŒì´ ê¸°ë°˜ ì¼)  \\n'\n",
      " '- ë©”íƒ€ ë½ì¹´  \\n'\n",
      " '- ì•ˆìŠ¤ë¡œí”½ í´ë¼ìš°ë“œ  \\n'\n",
      " '- ì—ì´ì•„ì´21 ìŠ¤ëˆ„í   \\n'\n",
      " '- ì¹´ì¹´í† ì˜ ì½”ë„  \\n'\n",
      " '- ë„¤ì´ë²„ í´ë¡œë°”X(í´ë¡œë“œ ê¸°ë°˜)  \\n'\n",
      " '- ì—…ìŠ¤í…Œì´ì§€ ì„¸ì†”(íŒŒì¸íŠœë‹ í´ë¼ë“œ)')\n"
     ]
    }
   ],
   "source": [
    "template_text = \"{model_name} ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ {count} ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\"\n",
    "\n",
    "# PromptTemplate ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "# í…œí”Œë¦¿ì— ê°’ì„ ì±„ì›Œì„œ í”„ë¡¬í”„íŠ¸ë¥¼ ì™„ì„±\n",
    "#filled_prompt = prompt_template.format(model_name=\"ChatGPT\", count=3)\n",
    "\n",
    "# ë¬¸ìì—´ í…œí”Œë¦¿ ê²°í•© (PromptTemplate + PromptTemplate + ë¬¸ìì—´)\n",
    "combined_prompt = (\n",
    "              prompt_template\n",
    "              + PromptTemplate.from_template(\"\\n\\n ê·¸ë¦¬ê³  {model_name} ëª¨ë¸ì˜ ì¥ì ì„ ìš”ì•½ ì •ë¦¬í•´ ì£¼ì„¸ìš”\")\n",
    "              + \"\\n\\n {model_name} ëª¨ë¸ê³¼ ë¹„ìŠ·í•œ AI ëª¨ë¸ì€ ì–´ë–¤ ê²ƒì´ ìˆë‚˜ìš”? ëª¨ë¸ëª…ì€ {language}ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\"\n",
    ")\n",
    "combined_prompt.format(model_name=\"ChatGPT\", count=3, language=\"í•œêµ­ì–´\")\n",
    "print(combined_prompt)\n",
    "\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "chain = combined_prompt | llm | StrOutputParser()\n",
    "response = chain.invoke({\"model_name\":\"ChatGPT\", \"count\":3, \"language\":\"í•œêµ­ì–´\"})\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PromptTemplate ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ë°°ì—´ í˜•íƒœë¡œ í•˜ì—¬ ì—¬ëŸ¬ê°œ ì‚¬ìš©í•˜ëŠ” ê²½ìš°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4, 16, 36, 64]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List Comprehension ì˜ˆì œ\n",
    "# before (ì ìš©í•˜ì§€ ì•Šì€ ê²½ìš°)\n",
    "result_list = []\n",
    "for val in range(10):\n",
    "    # 2ì˜\n",
    "    if val % 2 == 0:\n",
    "        result_list.append(val ** 2)\n",
    "\n",
    "result_list        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4, 16, 36, 64]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after (ì ìš©í•œ ê²½ìš°)\n",
    "result_list2 = [val**2 for val in range(10) if val % 2 == 0]\n",
    "result_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_text = \"{model_name} ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ {count} ë¬¸ì¥ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\"\n",
    "\n",
    "# PromptTemplate ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "questions = [\n",
    "    {\"model_name\": \"GPT-4\", \"count\": 3}, #dict\n",
    "    {\"model_name\": \"Gemini\", \"count\": 4},\n",
    "    {\"model_name\": \"claude\", \"count\": 4},\n",
    "]\n",
    "\n",
    "# ì—¬ëŸ¬ ê°œì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ë¯¸ë¦¬ ìƒì„±\n",
    "# që³€ìˆ˜ê°€ dict íƒ€ì…\n",
    "formatted_prompts = [prompt_template.format(**q) for q in questions]\n",
    "print(formatted_prompts)  # ë¯¸ë¦¬ ìƒì„±ëœ ì§ˆë¬¸ ëª©ë¡ í™•ì¸\n",
    "\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "for prompt in formatted_prompts:\n",
    "    print(type(prompt), prompt)\n",
    "    response = llm.invoke(prompt)\n",
    "    pprint(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) ChatPromptTemplate\n",
    "* Tuple í˜•íƒœì˜ system, user, assistant ë©”ì‹œì§€ ì§€ì›\n",
    "* ì—¬ëŸ¬ ê°œì˜ ë©”ì‹œì§€ë¥¼ ì¡°í•©í•˜ì—¬ LLMì—ê²Œ ì „ë‹¬ ê°€ëŠ¥\n",
    "* ê°„ê²°ì„±ê³¼ ê°€ë…ì„±ì´ ë†’ê³  ë‹¨ìˆœí•œ êµ¬ì¡°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-íŠœí”Œ í˜•íƒœì˜ ë©”ì‹œì§€ ëª©ë¡ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ìƒì„± (type, content)\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    # role, message\n",
    "    (\"system\", \"This system is an expert in answering questions about {topic}. \\\n",
    "     Please provide clear and detailed explanations.\"),\n",
    "    (\"user\", \"{model_name} ëª¨ë¸ì˜ í•™ìŠµ ì›ë¦¬ë¥¼ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\"),\n",
    "])\n",
    "\n",
    "messages = chat_prompt.format_messages(topic=\"AI\", model_name=\"ChatGPT\")\n",
    "print(messages)\n",
    "\n",
    "# ìƒì„±í•œ ë©”ì‹œì§€ë¥¼ ë°”ë¡œ ì£¼ì…í•˜ì—¬ í˜¸ì¶œí•˜ê¸°\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "print(type(response))\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì²´ì¸ì„ ìƒì„±í•˜ì—¬ í˜¸ì¶œí•˜ê¸°\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "chain = chat_prompt | llm | StrOutputParser()\n",
    "\n",
    "response = chain.invoke({\"topic\":\"AI\", \"model_name\":\"ChatGPT\"})\n",
    "print(type(response))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) ChatPromptTemplate\n",
    "* SystemMessagePromptTemplateì™€ HumanMessagePromptTemplate í´ë˜ìŠ¤ ì‚¬ìš©\n",
    "* ê°ì²´ ì§€í–¥ì  ì ‘ê·¼ - Message ê°ì²´ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ìƒì„± ê°€ëŠ¥\n",
    "* ì—¬ëŸ¬ ì¡°ê±´ì— ë”°ë¼ ë‹¤ë¥¸ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì„ íƒ\n",
    "\n",
    "```python\n",
    "if user_is_beginner:\n",
    "    system_message = SystemMessagePromptTemplate.from_template(\"ì´ˆë³´ìë¥¼ ìœ„í•œ ì„¤ëª…: {topic}\")\n",
    "else:\n",
    "    system_message = SystemMessagePromptTemplate.from_template(\"ì „ë¬¸ê°€ë¥¼ ìœ„í•œ ìƒì„¸ ë¶„ì„: {topic}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatMessagePromptTemplate í™œìš©\n",
    "\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    ChatMessagePromptTemplate\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ê°œë³„ ë©”ì‹œì§€ í…œí”Œë¦¿ ì •ì˜\n",
    "system_message = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an AI expert in {topic}. Please provide clear and detailed explanations.\"\n",
    ")\n",
    "user_message = HumanMessagePromptTemplate.from_template(\n",
    "    \"{question}\"\n",
    ")\n",
    "ai_message = AIMessagePromptTemplate.from_template(\n",
    "    \"This is an example answer about {topic}.\"\n",
    ")\n",
    "\n",
    "# ChatPromptTemplateë¡œ ë©”ì‹œì§€ë“¤ì„ ë¬¶ê¸°\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_message,\n",
    "    user_message,\n",
    "    ai_message\n",
    "])\n",
    "\n",
    "# ë©”ì‹œì§€ ìƒì„±\n",
    "messages = chat_prompt.format_messages(topic=\"ê±´ê°•\", question=\"ë…ê°ì˜ˆë°© ì ‘ì¢…ì„ ê¼­ ë§ì•„ì•¼ í•˜ë‚˜ìš”?\")\n",
    "\n",
    "# LLM í˜¸ì¶œ\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ChatMessagePromptTemplateëŠ” ì—¬ëŸ¬ ì¢…ë¥˜ì˜ ë©”ì‹œì§€(ì‹œìŠ¤í…œ, ì¸ê°„, AI)ë¥¼ ì¡°í•©í•˜ì—¬ ë³µì¡í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.\n",
    "* SystemMessagePromptTemplate: ì´ í…œí”Œë¦¿ì€ AI ëª¨ë¸ì—ê²Œ ì—­í• ì„ ë¶€ì—¬í•˜ê±°ë‚˜ ì „ë°˜ì ì¸ ê·œì¹™ì„ ì„¤ì •í•˜ëŠ” ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ë§Œë“­ë‹ˆë‹¤. ìœ„ì˜ ì˜ˆì‹œì—ì„œëŠ” \"ë²ˆì—­ì„ ë„ì™€ì£¼ëŠ” ìœ ìš©í•œ ë„ìš°ë¯¸\"ë¼ëŠ” ì—­í• ì„ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "* HumanMessagePromptTemplate: ì´ í…œí”Œë¦¿ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ë‚˜ ìš”ì²­ì„ ë‹´ëŠ” ì¸ê°„ ë©”ì‹œì§€ë¥¼ ë§Œë“­ë‹ˆë‹¤. ì•„ë˜ì˜ ì˜ˆì‹œì—ì„œëŠ” ë²ˆì—­í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ë°›ìŠµë‹ˆë‹¤.\n",
    "* ChatPromptTemplate.from_messages: ì´ í´ë˜ìŠ¤ ë©”ì„œë“œëŠ” ì‹œìŠ¤í…œ ë©”ì‹œì§€, ì¸ê°„ ë©”ì‹œì§€ ë“± ì—¬ëŸ¬ ì¢…ë¥˜ì˜ MessagePromptTemplate ê°ì²´ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°›ì•„ í•˜ë‚˜ì˜ ì±„íŒ… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ìœ¼ë¡œ í†µí•©í•©ë‹ˆë‹¤.\n",
    "* format_messages: ì´ ë©”ì„œë“œëŠ” ì •ì˜ëœ í…œí”Œë¦¿ì— ì‹¤ì œ ê°’ì„ ì±„ì›Œ ë„£ì–´ [SystemMessage, HumanMessage] í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. ì´ ë¦¬ìŠ¤íŠ¸ëŠ” ì±„íŒ… ëª¨ë¸(Chat Model) ì— ë°”ë¡œ ì „ë‹¬ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "# 1. SystemMessagePromptTemplateì™€ HumanMessagePromptTemplate ìƒì„±\n",
    "# SystemMessagePromptTemplateëŠ” ëª¨ë¸ì˜ í˜ë¥´ì†Œë‚˜ ë˜ëŠ” ê¸°ë³¸ ì§€ì¹¨ì„ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "system_template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "\n",
    "# HumanMessagePromptTemplateëŠ” ì‚¬ìš©ìë¡œë¶€í„° ë°›ëŠ” ì…ë ¥ í”„ë¡¬í”„íŠ¸ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "human_template = \"{text_to_translate}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "# 2. ChatPromptTemplate ìƒì„±\n",
    "# ìœ„ì—ì„œ ë§Œë“  ë‘ í…œí”Œë¦¿ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë¬¶ì–´ ChatPromptTemplateì„ ë§Œë“­ë‹ˆë‹¤.\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# 3. í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…\n",
    "# chat_prompt_template.format_messages()ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "# ì´ í•¨ìˆ˜ëŠ” ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ ì…ë ¥ ë³€ìˆ˜ë¥¼ ë°›ìŠµë‹ˆë‹¤.\n",
    "formatted_prompt = chat_prompt_template.format_messages(\n",
    "    input_language=\"English\",\n",
    "    output_language=\"Korean\",\n",
    "    text_to_translate=\"I love programming.\"\n",
    ")\n",
    "\n",
    "# 4. ê²°ê³¼ ì¶œë ¥\n",
    "print(formatted_prompt)\n",
    "\n",
    "# LLM í˜¸ì¶œ\n",
    "response = llm.invoke(formatted_prompt)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4) FewShotPromptTemplate\n",
    "* FewShotPromptTemplateì€ ëª¨ë¸ì´ íŠ¹ì • í˜•ì‹ì„ ë”°ë¥´ê²Œ í•˜ê±°ë‚˜, ì¼ê´€ëœ ì‘ë‹µì„ ìƒì„±í•˜ë„ë¡ ìœ ë„í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.\n",
    "* ë„ë©”ì¸ ì§€ì‹ì´ í•„ìš”í•˜ê±°ë‚˜, AIê°€ ì˜¤ë‹µì„ ì¤„ì´ê³  ë” ì‹ ë¢°í•  ë§Œí•œ ë‹µë³€ì„ ìƒì„±í•˜ë„ë¡ í•´ì•¼ í•  ë•Œ íš¨ê³¼ì ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4-1) PromptTemplateì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PromptTemplateì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# model\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# chain ì‹¤í–‰\n",
    "result = llm.invoke(\"íƒœì–‘ê³„ì˜ í–‰ì„±ë“¤ì„ ê°„ëµíˆ ì •ë¦¬í•´ ì£¼ì„¸ìš”.\")\n",
    "\n",
    "print(type(result))\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4-2) FewShotChatMessagePromptTemplate ì‚¬ìš©í•˜ëŠ” ê²½ìš°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x0000020C6B566FC0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000020C6B9D3E90> root_client=<openai.OpenAI object at 0x0000020C6B92FFB0> root_async_client=<openai.AsyncOpenAI object at 0x0000020C6BA2D2B0> model_name='moonshotai/kimi-k2-instruct-0905' temperature=0.7 model_kwargs={} openai_api_key=SecretStr('**********') openai_api_base='https://api.groq.com/openai/v1'\n",
      "first=ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='ë‹¹ì‹ ì€ ì´ˆë“±í•™ìƒë„ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì‰½ê²Œ ì„¤ëª…í•˜ëŠ” ê³¼í•™ êµìœ¡ìì…ë‹ˆë‹¤.'), additional_kwargs={}), FewShotChatMessagePromptTemplate(examples=[{'input': 'ë‰´í„´ì˜ ìš´ë™ ë²•ì¹™ì„ ìš”ì•½í•´ ì£¼ì„¸ìš”.', 'output': '### ë‰´í„´ì˜ ìš´ë™ ë²•ì¹™\\n1. **ê´€ì„±ì˜ ë²•ì¹™**: í˜ì´ ì‘ìš©í•˜ì§€ ì•Šìœ¼ë©´ ë¬¼ì²´ëŠ” ê³„ì† ê°™ì€ ìƒíƒœë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.\\n2. **ê°€ì†ë„ì˜ ë²•ì¹™**: ë¬¼ì²´ì— í˜ì´ ì‘ìš©í•˜ë©´, í˜ê³¼ ì§ˆëŸ‰ì— ë”°ë¼ ê°€ì†ë„ê°€ ê²°ì •ë©ë‹ˆë‹¤.\\n3. **ì‘ìš©-ë°˜ì‘ìš© ë²•ì¹™**: ëª¨ë“  í˜ì—ëŠ” í¬ê¸°ê°€ ê°™ê³  ë°©í–¥ì´ ë°˜ëŒ€ì¸ í˜ì´ ì‘ìš©í•©ë‹ˆë‹¤.'}, {'input': 'ì§€êµ¬ì˜ ëŒ€ê¸° êµ¬ì„± ìš”ì†Œë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.', 'output': '### ì§€êµ¬ ëŒ€ê¸°ì˜ êµ¬ì„±\\n- **ì§ˆì†Œ (78%)**: ëŒ€ê¸°ì˜ ëŒ€ë¶€ë¶„ì„ ì°¨ì§€í•©ë‹ˆë‹¤.\\n- **ì‚°ì†Œ (21%)**: ìƒëª…ì²´ê°€ í˜¸í¡í•˜ëŠ” ë° í•„ìš”í•©ë‹ˆë‹¤.\\n- **ì•„ë¥´ê³¤ (0.93%)**: ë°˜ì‘ì„±ì´ ë‚®ì€ ê¸°ì²´ì…ë‹ˆë‹¤.\\n- **ì´ì‚°í™”íƒ„ì†Œ (0.04%)**: ê´‘í•©ì„± ë° ì˜¨ì‹¤ íš¨ê³¼ì— ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.'}], input_variables=[], input_types={}, partial_variables={}, example_prompt=ChatPromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], input_types={}, partial_variables={}, template='{output}'), additional_kwargs={})])), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]) middle=[] last=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000020C6B566FC0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000020C6B9D3E90>, root_client=<openai.OpenAI object at 0x0000020C6B92FFB0>, root_async_client=<openai.AsyncOpenAI object at 0x0000020C6BA2D2B0>, model_name='moonshotai/kimi-k2-instruct-0905', temperature=0.7, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://api.groq.com/openai/v1')\n",
      "ì „ê¸°ë¥¼ ë§Œë“œëŠ” ê°€ì¥ ì‰¬ìš´ ë°©ë²•ì€ â€˜ìì„ìœ¼ë¡œ ì „ì„ ì„ í†¡í†¡ ê±´ë“œë¦¬ëŠ” ê²ƒâ€™ì´ì—ìš”!\n",
      "\n",
      "1. **ìì„ê³¼ ì „ì„ ì´ ë§Œë‚˜ë©´ ì „ê¸°ê°€ ìƒê²¨ìš”.**  \n",
      "   - ìì„ ì£¼ìœ„ì—ëŠ” ë³´ì´ì§€ ì•ŠëŠ” â€˜ìê¸°ì¥â€™ì´ë¼ëŠ” í˜ì´ ìˆì–´ìš”.  \n",
      "   - ì´ ìê¸°ì¥ ì•ˆì—ì„œ ì „ì„ ì„ ë¹ ë¥´ê²Œ ì›€ì§ì´ë©´ ì „ê¸°ê°€ ìƒê²¨ìš”(ì´ê±¸ â€˜ì „ìê¸° ìœ ë„â€™ë¼ê³  í•´ìš”).\n",
      "\n",
      "2. **ë°œì „ì†ŒëŠ” ì´ ì›ë¦¬ë¥¼ ì´ìš©í•´ìš”.**  \n",
      "   - ìˆ˜ë ¥ ë°œì „ì†Œ: ë¬¼ì´ íë¥´ë©° ë¬¼ë ˆë°©ì•„(í„°ë¹ˆ)ë¥¼ ëŒë¦¬ê³ , í„°ë¹ˆì´ ìì„ ì˜†ì˜ ì „ì„ ì„ ê³„ì† ì›€ì§ì—¬ ì „ê¸°ë¥¼ ë§Œë“¤ì–´ìš”.  \n",
      "   - í™”ë ¥ ë°œì „ì†Œ: ì„ìœ ë‚˜ ì„íƒ„ì„ íƒœì›Œ ëœ¨ê±°ì§„ ê¹€ì„ ë§Œë“¤ê³ , ì´ ê¹€ìœ¼ë¡œ í„°ë¹ˆì„ ëŒë ¤ìš”.  \n",
      "   - í’ë ¥Â·íƒœì–‘ê´‘ë„ ê°™ì€ ëª©ì ì„ ìœ„í•´ í„°ë¹ˆì´ë‚˜ ë°˜ë„ì²´ë¥¼ ì¨ì„œ ì „ê¸°ë¥¼ ë§Œë“¤ì–´ìš”.\n",
      "\n",
      "3. **ê²°ë¡ **  \n",
      "   â€˜ìì„ ì£¼ìœ„ì—ì„œ ì „ì„ ì„ ì›€ì§ì´ë©´â€™ ì „ê¸°ê°€ ìƒê¸°ê³ , ë°œì „ì†ŒëŠ” ê·¸ ì›€ì§ì„ì„ ê³„ì† ìœ ì§€í•´ ìš°ë¦¬ ì§‘ì— ì „ê¸°ë¥¼ ë³´ë‚´ìš”!\n"
     ]
    }
   ],
   "source": [
    "# FewShotChatMessagePromptTemplate ì‚¬ìš©í•˜ëŠ” ê²½ìš°\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"ë‰´í„´ì˜ ìš´ë™ ë²•ì¹™ì„ ìš”ì•½í•´ ì£¼ì„¸ìš”.\",\n",
    "        \"output\": \"\"\"### ë‰´í„´ì˜ ìš´ë™ ë²•ì¹™\n",
    "1. **ê´€ì„±ì˜ ë²•ì¹™**: í˜ì´ ì‘ìš©í•˜ì§€ ì•Šìœ¼ë©´ ë¬¼ì²´ëŠ” ê³„ì† ê°™ì€ ìƒíƒœë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.\n",
    "2. **ê°€ì†ë„ì˜ ë²•ì¹™**: ë¬¼ì²´ì— í˜ì´ ì‘ìš©í•˜ë©´, í˜ê³¼ ì§ˆëŸ‰ì— ë”°ë¼ ê°€ì†ë„ê°€ ê²°ì •ë©ë‹ˆë‹¤.\n",
    "3. **ì‘ìš©-ë°˜ì‘ìš© ë²•ì¹™**: ëª¨ë“  í˜ì—ëŠ” í¬ê¸°ê°€ ê°™ê³  ë°©í–¥ì´ ë°˜ëŒ€ì¸ í˜ì´ ì‘ìš©í•©ë‹ˆë‹¤.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"ì§€êµ¬ì˜ ëŒ€ê¸° êµ¬ì„± ìš”ì†Œë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.\",\n",
    "        \"output\": \"\"\"### ì§€êµ¬ ëŒ€ê¸°ì˜ êµ¬ì„±\n",
    "- **ì§ˆì†Œ (78%)**: ëŒ€ê¸°ì˜ ëŒ€ë¶€ë¶„ì„ ì°¨ì§€í•©ë‹ˆë‹¤.\n",
    "- **ì‚°ì†Œ (21%)**: ìƒëª…ì²´ê°€ í˜¸í¡í•˜ëŠ” ë° í•„ìš”í•©ë‹ˆë‹¤.\n",
    "- **ì•„ë¥´ê³¤ (0.93%)**: ë°˜ì‘ì„±ì´ ë‚®ì€ ê¸°ì²´ì…ë‹ˆë‹¤.\n",
    "- **ì´ì‚°í™”íƒ„ì†Œ (0.04%)**: ê´‘í•©ì„± ë° ì˜¨ì‹¤ íš¨ê³¼ì— ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# ì˜ˆì œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# FewShotChatMessagePromptTemplate ì ìš©\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "# ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ë‹¹ì‹ ì€ ì´ˆë“±í•™ìƒë„ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì‰½ê²Œ ì„¤ëª…í•˜ëŠ” ê³¼í•™ êµìœ¡ìì…ë‹ˆë‹¤.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ ìƒì„± ë° ì²´ì¸ êµ¬ì„±\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "chain = final_prompt | llm\n",
    "print(llm)\n",
    "print(chain)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "result = chain.invoke({\"input\": \"ì „ê¸°ì˜ ë°œìƒ ì›ë¦¬ì— ëŒ€í•˜ì—¬ ì„¤ëª…í•´ ì¤˜\"})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-1) PartialPrompt \n",
    "* í”„ë¡¬í”„íŠ¸ë¥¼ ë” ë™ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆìœ¼ë©°, AI ì‘ë‹µì„ ë” ì¼ê´€ì„± ìˆê²Œ ì¡°ì • ê°€ëŠ¥í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ê³„ì ˆì„ ê²°ì •í•˜ëŠ” í•¨ìˆ˜ (ë‚¨ë°˜êµ¬/ë¶ë°˜êµ¬ ê³ ë ¤)\n",
    "def get_current_season(hemisphere=\"north\"):\n",
    "    month = datetime.now().month\n",
    "\n",
    "    if hemisphere == \"north\":  # ë¶ë°˜êµ¬ (ê¸°ë³¸ê°’)\n",
    "        if 3 <= month <= 5:\n",
    "            return \"ë´„\"\n",
    "        elif 6 <= month <= 8:\n",
    "            return \"ì—¬ë¦„\"\n",
    "        elif 9 <= month <= 11:\n",
    "            return \"ê°€ì„\"\n",
    "        else:\n",
    "            return \"ê²¨ìš¸\"\n",
    "    else:  # ë‚¨ë°˜êµ¬ (ê³„ì ˆ ë°˜ëŒ€)\n",
    "        if 3 <= month <= 5:\n",
    "            return \"ê°€ì„\"\n",
    "        elif 6 <= month <= 8:\n",
    "            return \"ê²¨ìš¸\"\n",
    "        elif 9 <= month <= 11:\n",
    "            return \"ë´„\"\n",
    "        else:\n",
    "            return \"ì—¬ë¦„\"\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ (ë¶€ë¶„ ë³€ìˆ˜ ì ìš©)\n",
    "prompt = PromptTemplate(\n",
    "    template=\"{season}ì— ì¼ì–´ë‚˜ëŠ” ëŒ€í‘œì ì¸ ì§€êµ¬ê³¼í•™ í˜„ìƒì€ {phenomenon}ì´ ë§ë‚˜ìš”? \\\n",
    "        {season}ì— ì£¼ë¡œ ë°œìƒí•˜ëŠ” ì§€êµ¬ê³¼í•™ í˜„ìƒì„ 3ê°œ ì•Œë ¤ì£¼ì„¸ìš”\",\n",
    "    input_variables=[\"phenomenon\"],  # ì‚¬ìš©ì ì…ë ¥ í•„ìš”\n",
    "    partial_variables={\"season\": get_current_season()}  # ë™ì ìœ¼ë¡œ ê³„ì ˆ ê°’ í• ë‹¹\n",
    ")\n",
    "\n",
    "# OpenAI ëª¨ë¸ ì´ˆê¸°í™”\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "# íŠ¹ì • ê³„ì ˆì˜ í˜„ìƒ ì§ˆì˜\n",
    "query = prompt.format(phenomenon=\"íƒœí’ ë°œìƒ\")\n",
    "result = llm.invoke(query)\n",
    "\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(f\" í”„ë¡¬í”„íŠ¸: {query}\")\n",
    "print(f\" ëª¨ë¸ ì‘ë‹µ: {result.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í˜„ì¬ ê³„ì ˆ: ê²¨ìš¸\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# ê³„ì ˆì„ ê²°ì •í•˜ëŠ” í•¨ìˆ˜ (ë‚¨ë°˜êµ¬/ë¶ë°˜êµ¬ ê³ ë ¤)\n",
    "def get_current_season(hemisphere=\"north\"):\n",
    "    month = datetime.now().month\n",
    "\n",
    "    if hemisphere == \"north\":  # ë¶ë°˜êµ¬ (ê¸°ë³¸ê°’)\n",
    "        if 3 <= month <= 5:\n",
    "            return \"ë´„\"\n",
    "        elif 6 <= month <= 8:\n",
    "            return \"ì—¬ë¦„\"\n",
    "        elif 9 <= month <= 11:\n",
    "            return \"ê°€ì„\"\n",
    "        else:\n",
    "            return \"ê²¨ìš¸\"\n",
    "    else:  # ë‚¨ë°˜êµ¬ (ê³„ì ˆ ë°˜ëŒ€)\n",
    "        if 3 <= month <= 5:\n",
    "            return \"ê°€ì„\"\n",
    "        elif 6 <= month <= 8:\n",
    "            return \"ê²¨ìš¸\"\n",
    "        elif 9 <= month <= 11:\n",
    "            return \"ë´„\"\n",
    "        else:\n",
    "            return \"ì—¬ë¦„\"\n",
    "\n",
    "# Step 1: í˜„ì¬ ê³„ì ˆ ê²°ì •\n",
    "season_name = get_current_season()  # ê³„ì ˆ ê°’ ì–»ê¸°\n",
    "print(f\"í˜„ì¬ ê³„ì ˆ: {season_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: í•´ë‹¹ ê³„ì ˆì˜ ìì—° í˜„ìƒ ì¶”ì²œ\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"{season}ì— ì£¼ë¡œ ë°œìƒí•˜ëŠ” ëŒ€í‘œì ì¸ ì§€êµ¬ê³¼í•™ í˜„ìƒ 3ê°€ì§€ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”. \"\n",
    "    \"ê° í˜„ìƒì— ëŒ€í•´ ê°„ë‹¨í•œ ì„¤ëª…ì„ í¬í•¨í•´ì£¼ì„¸ìš”.\"\n",
    ")\n",
    "\n",
    "# OpenAI ëª¨ë¸ ì‚¬ìš©\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "# llm = ChatOpenAI(\n",
    "#     #api_key=OPENAI_API_KEY,\n",
    "#     base_url=\"https://api.groq.com/openai/v1\",  # Groq API ì—”ë“œí¬ì¸íŠ¸\n",
    "#     model=\"meta-llama/llama-4-scout-17b-16e-instruct\",  # Spring AIì™€ ë™ì¼í•œ ëª¨ë¸\n",
    "#     temperature=0.0\n",
    "# )\n",
    "\n",
    "# ì²´ì¸ 2: ìì—° í˜„ìƒ ì¶”ì²œ (ì…ë ¥: ê³„ì ˆ â†’ ì¶œë ¥: ìì—° í˜„ìƒ ëª©ë¡)\n",
    "chain2 = (\n",
    "    {\"season\": lambda x : season_name}  # chain1ì˜ ì¶œë ¥ì„ season ë³€ìˆ˜ë¡œ ì „ë‹¬\n",
    "    | prompt2\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# ì‹¤í–‰: í˜„ì¬ ê³„ì ˆì— ë”°ë¥¸ ìì—° í˜„ìƒ ì¶”ì²œ\n",
    "response = chain2.invoke({})\n",
    "print(f\"\\n {season_name}ì— ë°œìƒí•˜ëŠ” ìì—° í˜„ìƒ:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5-2) PartialPrompt \n",
    "* API í˜¸ì¶œ ë°ì´í„°, ì‹œê°„ ì •ë³´, ì‚¬ìš©ì ì •ë³´ ë“±ì„ ë°˜ì˜í•  ë•Œ ë§¤ìš° ìœ ìš©í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " í”„ë¡¬í”„íŠ¸: í˜„ì¬ 1ë‹¬ëŸ¬ = 1472.71ì› ê¸°ì¤€ìœ¼ë¡œ í™˜ìœ¨ ì •ë³´ë¥¼ ì•Œë ¤ë“œë¦½ë‹ˆë‹¤. ì´ì— ëŒ€í•œ ë¶„ì„ì„ ì œê³µí•´ ì£¼ì„¸ìš”.\n",
      " ëª¨ë¸ ì‘ë‹µ: í˜„ì¬ í™˜ìœ¨ **1 USD = 1,472.71 KRW**ëŠ” **ì—­ëŒ€ ìµœê³  ìˆ˜ì¤€**ì— ê·¼ì ‘í•œ ìˆ˜ì¹˜ë¡œ, **ì‹¬ë¦¬ì  ì €í•­ì„  1,500ì›**ì´ ì½”ì•ì— ìˆëŠ” ìƒí™©ì…ë‹ˆë‹¤. ì•„ë˜ì— ê±°ì‹œê²½ì œì , ì •ì¹˜ì , ì‹œì¥ ì‹¬ë¦¬ì  ê´€ì ì—ì„œì˜ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "### ğŸ” 1. **í™˜ìœ¨ ê¸‰ë“±ì˜ ì£¼ìš” ì›ì¸**\n",
      "\n",
      "| ìš”ì¸ | ì„¤ëª… |\n",
      "|------|------|\n",
      "| **ë¯¸êµ­ ê¸ˆë¦¬ ê³ ì  ì§€ì†** | ë¯¸ ì—°ì¤€ì€ ì¸í”Œë ˆì´ì…˜ ì™„í™”è¿¹è±¡ì—ë„ ë¶ˆêµ¬í•˜ê³  **ê¸ˆë¦¬ ì¸í•˜ë¥¼ ë¯¸ë£¨ê³  ìˆìŒ**. ì´ë¡œ ì¸í•´ **ë‹¬ëŸ¬ ê°•ì„¸**ê°€ ì§€ì†ë˜ë©°, ì›í™”ëŠ” ì•½ì„¸ë¡œ ë°˜ì‘. |\n",
      "| **í•œêµ­ ê²½ê¸° ë‘”í™”** | í•œêµ­ì˜ **ìˆ˜ì¶œ ê°ì†Œ + ë‚´ìˆ˜ ì¹¨ì²´**ë¡œ ì„±ì¥ë¥  ì „ë§ì´ í•˜í–¥ ì¡°ì •ë˜ë©°, ì›í™” ì•½ì„¸ ì••ë ¥ ì¦ê°€. |\n",
      "| **ì¤‘êµ­ ê²½ê¸° ì¹¨ì²´** | ì¤‘êµ­ì˜ ë¶€ì§„í•œ ê²½ì œ ì§€í‘œëŠ” í•œêµ­ ìˆ˜ì¶œì— ì§ì ‘ì  íƒ€ê²©. ì¤‘êµ­ì€ í•œêµ­ ìˆ˜ì¶œì˜ ì•½ 25%ë¥¼ ì°¨ì§€. |\n",
      "| **ì§€ì •í•™ì  ë¦¬ìŠ¤í¬** | ëŸ¬ì‹œì•„-ìš°í¬ë¼ì´ë‚˜, ì¤‘ë™ ê¸´ì¥, ëŒ€ë§Œ í•´í˜‘ ë¶ˆì•ˆ ë“±ìœ¼ë¡œ **ì•ˆì „ìì‚° ì„ í˜¸** â†’ ë‹¬ëŸ¬ ìˆ˜ìš” ì¦ê°€. |\n",
      "| **ì›í™” ì•½ì„¸ ì reinforcing** | ìˆ˜ì… ê¸°ì—…ë“¤ì˜ **í—¤ì§€ ë¬¼ëŸ‰ ì¦ê°€**, ê°œì¸ì˜ **ë‹¬ëŸ¬ ì‚¬ì¬ê¸° ì‹¬ë¦¬** ìœ ë°œ â†’ **ìì¶©ìˆ˜ìš”** ë°œìƒ. |\n",
      "\n",
      "---\n",
      "\n",
      "### ğŸ“Š 2. **1,472.71ì›ì´ ì˜ë¯¸í•˜ëŠ” ë°”**\n",
      "\n",
      "| ì§€í‘œ | ì˜ë¯¸ |\n",
      "|------|------|\n",
      "| **ì—­ëŒ€ ìµœê³ ì¹˜ ëŒ€ë¹„** | 2022ë…„ 10ì›” 1ì°¨ ê³ ì (1,452.50ì›)ì„ ë›°ì–´ë„˜ìŒ. **ì‚¬ìƒ ìµœê³ ì¹˜ëŠ” 2022ë…„ 11ì›” 1,576ì›**. |\n",
      "| **ì—°ê°„ ë³€ë™í­** | 2023ë…„ ì´ˆ ëŒ€ë¹„ ì•½ **13% ìƒìŠ¹**. |\n",
      "| **ì‹¤ì§ˆí™˜ìœ¨(NER)** | **ì‹¤ì§ˆí™˜ìœ¨ì€ 1,300ì›ëŒ€ ì¤‘ë°˜**ìœ¼ë¡œ, **ëª…ëª©í™˜ìœ¨ì´ ì‹¤ì§ˆí™˜ìœ¨ë³´ë‹¤ ë†’ìŒ** â†’ **ì›í™” ì‹¤ì§ˆ ê³¼ì €í‰ê°€** ìƒíƒœ. |\n",
      "\n",
      "---\n",
      "\n",
      "### âš ï¸ 3. **í–¥í›„ ì „ë§ (ë² ì´ìŠ¤ë¼ì¸ ì‹œë‚˜ë¦¬ì˜¤)**\n",
      "\n",
      "| ê¸°ê°„ | ì „ë§ |\n",
      "|------|------|\n",
      "| **ë‹¨ê¸° (1~3ê°œì›”)** | **1,480~1,520ì›** ë°•ìŠ¤ê¶Œ íë¦„. ë¯¸ ì—°ì¤€ì˜ **ê¸ˆë¦¬ ì¸í•˜ ì‹œì **ì´ í•µì‹¬ ë³€ìˆ˜. |\n",
      "| **ì¤‘ê¸° (3~6ê°œì›”)** | **í•œêµ­ ê²½ê¸° ë°˜ë“± + ìˆ˜ì¶œ íšŒë³µ** ì‹œ **1,400ì›ëŒ€ ì´ˆë°˜**ìœ¼ë¡œ í•˜ë½ ê°€ëŠ¥. ê·¸ëŸ¬ë‚˜ **ì¤‘êµ­ ê²½ê¸° íšŒë³µ ì†ë„**ê°€ í•µì‹¬. |\n",
      "| **ì¥ê¸° (6~12ê°œì›”)** | **ë¯¸ ê¸ˆë¦¬ ì¸í•˜ ë³¸ê²©í™” + í•œêµ­ ê²½ê¸° ë°˜ë“±** ì‹œ **1,300ì›ëŒ€ ì¤‘ë°˜**ìœ¼ë¡œ í•˜ë½ ê°€ëŠ¥. ê·¸ëŸ¬ë‚˜ **1,300ì› ì´í•˜ëŠ” êµ¬ì¡°ì ìœ¼ë¡œ ì–´ë ¤ì›€** (ì—ë„ˆì§€ ìˆ˜ì… ì˜ì¡´ë„, ê³ ë¬¼ê°€ êµ¬ì¡°).\n",
      "\n",
      "---\n",
      "\n",
      "### ğŸ“Œ 4. **íˆ¬ìì/ì†Œë¹„ì ëŒ€ì‘ ì „ëµ**\n",
      "\n",
      "| ì£¼ì²´ | ì „ëµ |\n",
      "|------|------|\n",
      "| **ìˆ˜ì… ê¸°ì—…** | **ë‹¬ëŸ¬ ì„ ë¬¼ í™˜ë§¤** ê³ ë ¤. ë‹¨ê¸° í™˜ìœ¨ ê¸‰ë“± ì‹œ **ì˜µì…˜ í—¤ì§€** í™œìš©. |\n",
      "| **ìˆ˜ì¶œ ê¸°ì—…** | **ë‹¬ëŸ¬ ë§¤ë„ í¬ì§€ì…˜** ì¼ë¶€ í™•ëŒ€. ê·¸ëŸ¬ë‚˜ **ì¤‘êµ­ ìˆ˜ìš” ë‘”í™”** ê°ì•ˆí•´ **ê³¼ë„í•œ ë ˆë²„ë¦¬ì§€ ê¸ˆë¬¼**. |\n",
      "| **ê°œì¸ íˆ¬ìì** | **ë‹¬ëŸ¬ ì˜ˆê¸ˆ**ì€ **í™˜ìœ¨ 1,500ì› ì´ìƒ**ì—ì„œ ì ì§„ì  ë§¤ë„ ê³ ë ¤. **ì›í™” ìì‚°**ì€ **1,400ì› ì´í•˜**ì—ì„œ ë¶„í•  ë§¤ìˆ˜ ìœ ë¦¬. |\n",
      "| **ì†Œë¹„ì** | **í•´ì™¸ ì§êµ¬, ì—¬í–‰**ì€ **í™˜ìœ¨ í•˜ë½ ì‹œ** ì‹¤í–‰. **ë‹¬ëŸ¬ ì‚¬ì¬ê¸°**ëŠ” **êµ¬ì¡°ì  ë¦¬ìŠ¤í¬** ì¡´ì¬ â†’ **ìì œ ê¶Œê³ **.\n",
      "\n",
      "---\n",
      "\n",
      "### ğŸ§­ 5. **í•µì‹¬ ëª¨ë‹ˆí„°ë§ ì§€í‘œ**\n",
      "\n",
      "| ì§€í‘œ | ì„ê³„ì¹˜ | ì˜ë¯¸ |\n",
      "|------|--------|------|\n",
      "| **ë¯¸ 10ë…„ë¬¼ ê¸ˆë¦¬** | **4.5% ì´ìƒ** | ë‹¬ëŸ¬ ê°•ì„¸ ì§€ì† |\n",
      "| **í•œêµ­ ìˆ˜ì¶œ (-YoY)** | **-10% ì´í•˜** | ì›í™” ì•½ì„¸ ì••ë ¥ |\n",
      "| **ì¤‘êµ­ PMI** | **50 ì´í•˜** | í•œêµ­ ìˆ˜ì¶œ ì•…í™” |\n",
      "| **ì›/ì—” í™˜ìœ¨** | **1,000ì› ì´ìƒ** | **ìºë¦¬íŠ¸ë ˆì´ë“œ** ìœ ë°œ â†’ ì›í™” ì¶”ê°€ ì•½ì„¸ |\n",
      "\n",
      "---\n",
      "\n",
      "### âœ… ê²°ë¡ \n",
      "\n",
      "> **1,472.71ì›ì€ ì‹¬ë¦¬ì  ì €í•­ì„  1,500ì›ì„ ì½”ì•ì— ë‘” ìœ„í—˜ ìˆ˜ì¤€.**  \n",
      "> **ë‹¨ê¸°ì ìœ¼ë¡œëŠ” 1,500ì› ëŒíŒŒ ê°€ëŠ¥ì„± ìˆìœ¼ë‚˜, ì¤‘ê¸°ì ìœ¼ë¡œ í•œêµ­ ê²½ê¸° ë°˜ë“± + ë¯¸ ê¸ˆë¦¬ ì¸í•˜ ì‹œ 1,300ì›ëŒ€ íšŒì‹  ê°€ëŠ¥.**  \n",
      "> **ê°œì¸ì€ ë‹¬ëŸ¬ ì‚¬ì¬ê¸°ë³´ë‹¤ëŠ” í™˜ìœ¨ ë³€ë™ì„±ì— ëŒ€í•œ ëŒ€ë¹„ì— ì§‘ì¤‘í•´ì•¼ í•¨.**\n",
      "\n",
      "---\n",
      "\n",
      "í•„ìš”í•˜ì‹œë©´ **í™˜ìœ¨ ì‹œë‚˜ë¦¬ì˜¤ ëª¨ë¸ë§**ì´ë‚˜ **ê°œì¸ í¬íŠ¸í´ë¦¬ì˜¤ í—¤ì§€ ì „ëµ**ë„ ë„ì™€ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ì‹¤ì‹œê°„ í™˜ìœ¨ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\n",
    "def get_exchange_rate():\n",
    "    response = requests.get(\"https://api.exchangerate-api.com/v4/latest/USD\")\n",
    "    data = response.json()\n",
    "    return f\"1ë‹¬ëŸ¬ = {data['rates']['KRW']}ì›\"\n",
    "\n",
    "# Partial Prompt í™œìš©\n",
    "prompt = PromptTemplate(\n",
    "    template=\"í˜„ì¬ {info} ê¸°ì¤€ìœ¼ë¡œ í™˜ìœ¨ ì •ë³´ë¥¼ ì•Œë ¤ë“œë¦½ë‹ˆë‹¤. ì´ì— ëŒ€í•œ ë¶„ì„ì„ ì œê³µí•´ ì£¼ì„¸ìš”.\",\n",
    "    input_variables=[],  # ì‚¬ìš©ì ì…ë ¥ ì—†ìŒ\n",
    "    partial_variables={\"info\": get_exchange_rate()}  # APIì—ì„œ ê°€ì ¸ì˜¨ ë°ì´í„° ìë™ ë°˜ì˜\n",
    ")\n",
    "\n",
    "# LLM ëª¨ë¸ ì„¤ì •\n",
    "#llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "\n",
    "# ëª¨ë¸ì— í”„ë¡¬í”„íŠ¸ ì „ë‹¬ ë° ì‘ë‹µ ë°›ê¸°\n",
    "response = llm.invoke(prompt.format())\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\" í”„ë¡¬í”„íŠ¸:\", prompt.format())\n",
    "print(\" ëª¨ë¸ ì‘ë‹µ:\", response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mylangchain-app-tQs1zK9u-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
